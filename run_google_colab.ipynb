{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/machinelearnear/open-hf-spaces-in-studiolab/blob/main/run_google_colab.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tA1VyFSQOAjd",
    "tags": []
   },
   "source": [
    "# `< Nombre >` a travÃ©s de Hugging Face Spaces\n",
    "\n",
    "`machinelearnear` ðŸ§‰ðŸ¤– > https://www.youtube.com/c/machinelearnear\n",
    "---\n",
    "**Referencias**\n",
    "- https://www.youtube.com/watch?v=vqdl31w-nWo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j9RkjQ99OIU_",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Configurar entorno de Hugging Face Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rmBjvPVHO1fs"
   },
   "outputs": [],
   "source": [
    "#@title Un poco de cÃ³digo para configurar todo, clonar el repo, instalar las librerias, etc.\n",
    "# source: https://www.youtube.com/c/machinelearnear\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import subprocess\n",
    "import re\n",
    "\n",
    "try:\n",
    "    import gradio as gr\n",
    "except ImportError:\n",
    "    subprocess.run([\"pip\", \"install\", \"gradio\"])\n",
    "    import gradio as gr\n",
    "\n",
    "from os.path import exists as path_exists\n",
    "from pathlib import Path\n",
    "from typing import Dict\n",
    "\n",
    "newline, bold, unbold = \"\\n\", \"\\033[1m\", \"\\033[0m\"\n",
    "\n",
    "class RepoHandler:\n",
    "    def __init__(self, repo_url: str) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the RepoHandler class with the provided repository URL and requirements file.\n",
    "        Args:\n",
    "        - repo_url: URL of the git repository to clone.\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        self.is_google_colab = False\n",
    "        self.fix_for_gr, self.fix_for_st = None, None\n",
    "        self.app_file = \"app.py\"\n",
    "        if 'google.colab' in str(get_ipython()):\n",
    "            print(f'{bold}Running on: \"Google Colab\"{unbold}')\n",
    "            self.is_google_colab = True\n",
    "        else:\n",
    "            print(f'{bold}Running on: Local or \"SM Studio Lab\"{unbold}')\n",
    "        self.repo_url = repo_url\n",
    "        self.repo_name = self.repo_url.split('/')[-1]\n",
    "\n",
    "    def __str__(self):\n",
    "        if os.path.exists(self.repo_name):\n",
    "            return self.retrieve_readme(f'{self.repo_name}/README.md')\n",
    "        else:\n",
    "            print(f\"{bold}The repo '{self.repo_name}' has not been cloned yet.{unbold}\")\n",
    "            return None\n",
    "            \n",
    "    def retrieve_readme(self, filename) -> Dict:\n",
    "        readme = {}\n",
    "        if path_exists(filename):\n",
    "            with open(filename) as f:\n",
    "                for line in f:\n",
    "                    if line.find('http') > 0: continue\n",
    "                    if not line.find(':') > 0 or 'Check' in line: continue\n",
    "                    (k,v) = line.split(':')\n",
    "                    readme[(k)] = v.strip().replace('\\n','')\n",
    "        else:\n",
    "            print(f\"{bold}No 'readme.md' file{unbold}\")\n",
    "            \n",
    "        return readme\n",
    "        \n",
    "    def clone_repo(self, overwrite=False) -> None:\n",
    "        \"\"\"\n",
    "        Clone the git repository specified in the repo_url attribute.\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        # Check if repository has already been cloned locally\n",
    "        if overwrite and os.path.exists(self.repo_name): \n",
    "            try:\n",
    "                shutil.rmtree(self.repo_name)\n",
    "            except OSError as e:\n",
    "                print(\"Error: %s - %s.\" % (e.filename, e.strerror))\n",
    "        if os.path.exists(self.repo_name):\n",
    "            print(f\"{bold}Repository '{self.repo_name}' has already been cloned.{unbold}\")\n",
    "        else:\n",
    "            print(f\"{bold}Cloning repo... may take a few minutes... remember to set your Space to 'public'...{unbold}\")\n",
    "            subprocess.run([\"apt-get\", \"install\", \"git-lfs\"])\n",
    "            subprocess.run([\"git\", \"lfs\", \"install\", \"--system\", \"--skip-repo\"])\n",
    "            subprocess.run([\"git\", \"clone\", \"--recurse-submodules\", self.repo_url])\n",
    "\n",
    "    def install_requirements(self, requirements_file: str = None, install_xformers: bool = False) -> None:\n",
    "        \"\"\"\n",
    "        Install the requirements specified in the requirements_file attribute.\n",
    "        \n",
    "        Args:\n",
    "        - requirements_file: Name of the file containing the requirements to install. This file must be \n",
    "        located in the root directory of the repository. Defaults to \"requirements.txt\".\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        if not requirements_file: requirements_file = f\"{self.repo_name}/requirements.txt\"\n",
    "        \n",
    "        # install requirements\n",
    "        print(f\"{bold}Installing requirements... may take a few minutes...{unbold}\")\n",
    "        subprocess.run([\"pip\", \"install\", \"-r\", requirements_file])\n",
    "        if install_xformers: self.install_xformers()\n",
    "\n",
    "    def run_web_demo(self, aws_domain=None, aws_region=None) -> None:\n",
    "        \"\"\"\n",
    "        Launch the Gradio or Streamlit web demo for the cloned repository.\n",
    "        Works with Google Colab or SageMaker Studio Lab.\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        import torch\n",
    "        if torch.cuda.is_available(): print(f\"{bold}Using: {unbold}{self.get_gpu_memory_map()}\")\n",
    "        else: print(f\"{bold}Not using the GPU{unbold}\")\n",
    "        \n",
    "        readme = self.__str__()\n",
    "        self.app_file = readme[\"app_file\"]\n",
    "        print(f\"{bold}Demo: `{readme['title']}`{newline}{unbold}\")\n",
    "        print(f\"{bold}Downloading models... might take up to 10 minutes to finish...{unbold}\")\n",
    "        print(f\"{bold}Once finished, click the link below to open your application (in SM Studio Lab):{newline}{unbold}\")\n",
    "        if all([aws_domain, aws_region]):\n",
    "              print(f'{bold}https://{aws_domain}.studio.{aws_region}.sagemaker.aws/studiolab/default/jupyter/proxy/6006/{unbold}')\n",
    "                \n",
    "        self.unset_environment_variables()\n",
    "        \n",
    "        if readme[\"sdk\"] == 'gradio':\n",
    "            gr.close_all()\n",
    "            if not self.is_google_colab:\n",
    "                !export GRADIO_SERVER_PORT=6006 && cd $self.repo_name && python $self.app_file\n",
    "                # os.system(f'export GRADIO_SERVER_PORT=6006 && cd {self.repo_name} && python {readme[\"app_file\"]}')\n",
    "            else:\n",
    "                new_filename = self.replace_gradio_launcher(f'{self.repo_name}/{readme[\"app_file\"]}')\n",
    "                !cd $self.repo_name && python $new_filename\n",
    "                # os.system(f'cd {self.repo_name} && python {readme[\"app_file\"]}')\n",
    "        elif readme[\"title\"] == 'streamlit':\n",
    "            if not self.is_google_colab:\n",
    "                !cd $self.repo_name && streamlit run $self.app_file --server.port 6006\n",
    "                # os.system(f'cd {self.repo_name} && streamlit run {readme[\"app_file\"]} --server.port 6006')\n",
    "            else:\n",
    "                !cd $self.repo_name && streamlit run $self.app_file\n",
    "                # os.system(f'cd {self.repo_name} && streamlit run {readme[\"app_file\"]}')\n",
    "        else:\n",
    "            print('This notebook will not work with static apps hosted on \"Spaces\"')\n",
    "\n",
    "    def get_gpu_memory_map(self) -> Dict[str, int]:\n",
    "        \"\"\"Get the current gpu usage.\n",
    "        Return:\n",
    "            A dictionary in which the keys are device ids as integers and\n",
    "            values are memory usage as integers in MB.\n",
    "        \"\"\"\n",
    "        result = subprocess.run(\n",
    "            [\"nvidia-smi\", \"--query-gpu=name,memory.total,memory.free\", \"--format=csv,noheader\",],\n",
    "            encoding=\"utf-8\",\n",
    "            # capture_output=True,          # valid for python version >=3.7\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,  # for backward compatibility with python version 3.6\n",
    "            check=True,\n",
    "        )\n",
    "        # Convert lines into a dictionary, return f\"{}\"\n",
    "        gpu_memory = [x for x in result.stdout.strip().split(os.linesep)]\n",
    "        gpu_memory_map = {f\"gpu_{index}\": memory for index, memory in enumerate(gpu_memory)}\n",
    "        \n",
    "        return gpu_memory_map\n",
    "\n",
    "    def replace_gradio_launcher(self, old_filename) -> str:\n",
    "        # Read the contents of the file\n",
    "        with open(old_filename, \"r\") as f:\n",
    "            contents = f.read()\n",
    "        # Define the regular expression pattern\n",
    "        pattern = r\"\\.launch\\((.*?)\\)\"\n",
    "        # Use the sub method to replace the text\n",
    "        contents = re.sub(pattern, \".launch(share=True)\", contents)\n",
    "        # Write the modified contents back to the file\n",
    "        new_filename = Path(old_filename).parent / f\"{Path(old_filename).stem}_modified.py\"\n",
    "        with open(new_filename, \"w\") as f:\n",
    "            f.write(contents)\n",
    "\n",
    "        return new_filename.name\n",
    "    \n",
    "    def unset_environment_variables(self) -> None:\n",
    "        os.unsetenv(\"SHARED_UI\")\n",
    "        os.environ.pop(\"SHARED_UI\", None)\n",
    "        \n",
    "        os.unsetenv(\"IS_SHARED\")\n",
    "        os.environ.pop(\"IS_SHARED\", None)\n",
    "\n",
    "    def install_xformers(self) -> None:\n",
    "        from subprocess import getoutput\n",
    "        from IPython.display import HTML\n",
    "        from IPython.display import clear_output\n",
    "        import time\n",
    "\n",
    "        subprocess.run([\"pip\", \"install\", \"-U\", \"--pre\", \"triton\"])\n",
    "\n",
    "        s = getoutput('nvidia-smi')\n",
    "        if 'T4' in s: gpu = 'T4'\n",
    "        elif 'P100' in s: gpu = 'P100'\n",
    "        elif 'V100' in s: gpu = 'V100'\n",
    "        elif 'A100' in s: gpu = 'A100'\n",
    "\n",
    "        while True:\n",
    "            try: \n",
    "                gpu=='T4'or gpu=='P100'or gpu=='V100'or gpu=='A100'\n",
    "                break\n",
    "            except:\n",
    "                pass\n",
    "            print(f'{bold} Seems that your GPU is not supported at the moment.{unbold}')\n",
    "            time.sleep(5)\n",
    "\n",
    "        if (gpu=='T4'): \n",
    "            precompiled_wheels = \"https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/T4/xformers-0.0.13.dev0-py3-none-any.whl\"\n",
    "        elif (gpu=='P100'): \n",
    "            precompiled_wheels = \"https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/P100/xformers-0.0.13.dev0-py3-none-any.whl\"\n",
    "        elif (gpu=='V100'): \n",
    "            precompiled_wheels = \"https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/V100/xformers-0.0.13.dev0-py3-none-any.whl\"\n",
    "        elif (gpu=='A100'): \n",
    "            precompiled_wheels = \"https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/A100/xformers-0.0.13.dev0-py3-none-any.whl\"\n",
    "\n",
    "        subprocess.run([\"pip\", \"install\", \"-q\", precompiled_wheels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ioW5IDk4PXg4"
   },
   "source": [
    "## Ya tenemos todo listo, empezar el proceso! ðŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DvACWIthPn4P",
    "outputId": "9ce736c1-5ef8-4737-e48b-8b27fa0c6797"
   },
   "outputs": [],
   "source": [
    "app = RepoHandler(\n",
    "    repo_url='https://huggingface.co/spaces/hysts/LoRA-SD-training'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xfE0Zx-2PvUd",
    "outputId": "073e29e0-40ba-4d47-d816-d0dceb98b6a9"
   },
   "outputs": [],
   "source": [
    "app.clone_repo(overwrite=False)\n",
    "app.install_requirements()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6tkTJI0SPvv5",
    "outputId": "e69a7612-5556-40c9-ada8-3bbac606f8a0"
   },
   "outputs": [],
   "source": [
    "app.run_web_demo()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "machinelearnear_sd_paint_by_example.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
